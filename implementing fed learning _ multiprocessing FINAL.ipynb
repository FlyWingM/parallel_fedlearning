{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from utils.customloader import CustomDataset, DatasetSplit\n",
    "from utils.separate_into_classes import separate_into_classes\n",
    "from utils.arguments import Args\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar\n",
    "from models.Fed import FedAvg\n",
    "from models.test import test_img\n",
    "from utils.smooth_crossentropy import SmoothCrossEntropyLoss\n",
    "from utils.dataloader import get_dataloader, set_seed\n",
    "import random\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_glob import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1, 28, 28)\n",
      "torch.Size([30000])\n",
      "(30000, 1, 28, 28)\n",
      "torch.Size([30000])\n",
      "Train Epoch: 0 [0/30000 (0%)]\tLoss: 2.348456\n",
      "\n",
      "Train loss: 2.306430689493815\n",
      "\n",
      "Test set: Average loss: 0.00226 \n",
      "Accuracy: 1493/10000 (14.93%)\n",
      "\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.268748\n",
      "\n",
      "Train loss: 2.247836192448934\n",
      "\n",
      "Test set: Average loss: 0.00218 \n",
      "Accuracy: 3681/10000 (36.81%)\n",
      "\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 2.210516\n",
      "\n",
      "Train loss: 2.142793337504069\n",
      "\n",
      "Test set: Average loss: 0.00196 \n",
      "Accuracy: 5032/10000 (50.32%)\n",
      "\n",
      "Train Epoch: 3 [0/30000 (0%)]\tLoss: 2.055670\n",
      "\n",
      "Train loss: 1.9120877504348754\n",
      "\n",
      "Test set: Average loss: 0.00156 \n",
      "Accuracy: 6293/10000 (62.93%)\n",
      "\n",
      "Train Epoch: 4 [0/30000 (0%)]\tLoss: 1.738179\n",
      "\n",
      "Train loss: 1.6306919535001119\n",
      "\n",
      "Test set: Average loss: 0.00117 \n",
      "Accuracy: 7517/10000 (75.17%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1, 28, 28)\n",
      "torch.Size([30000])\n",
      "(30000, 1, 28, 28)\n",
      "torch.Size([30000])\n"
     ]
    }
   ],
   "source": [
    "args = Args()    \n",
    "\n",
    "#torch.cuda.is_available()\n",
    "\n",
    "#set_seed(args.seed)\n",
    "global_train_loader, local_train_loader, test_loader = get_dataloader(data='mnist', \n",
    "                                                                      test_size=0.5, \n",
    "                                                                      num_workers=8,\n",
    "                                                                      batch_size=1000, \n",
    "                                                                      seed=args.seed,\n",
    "                                                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNMnist(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_glob = CNNMnist(args=args).to(args.device)\n",
    "net_glob.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "total_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "optimizer = optim.SGD(net_glob.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "sloss = F.cross_entropy\n",
    "#sloss = SmoothCrossEntropyLoss(smoothing=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_train_epoch(epoch, net_glob, global_train_loader, sloss, optimizer):    \n",
    "    net_glob.train()\n",
    "    batch_loss = []\n",
    "    for batch_idx,(data, target) in enumerate(global_train_loader):\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "        output = net_glob(data)\n",
    "        loss = sloss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(global_train_loader.dataset),\n",
    "                       100. * batch_idx / len(global_train_loader), loss.item()))\n",
    "        batch_loss.append(loss.item())\n",
    "    loss_avg = sum(batch_loss)/len(batch_loss)\n",
    "    print('\\nTrain loss:', loss_avg)\n",
    "    \n",
    "    return net_glob, loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net_glob, test_loader, sloss):\n",
    "    net_glob.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    l = len(test_loader)\n",
    "    for idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "        log_probs = net_glob(data)\n",
    "        test_loss += F.cross_entropy(log_probs, target).item()\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    #After fedlearning\n",
    "    #print('After Federated Learning')\n",
    "    print('\\nTest set: Average loss: {:.5f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_loss = []\n",
    "net_glob.train()\n",
    "for epoch in range(args.epochs):\n",
    "    net_glob, loss_avg = global_train_epoch(epoch, net_glob, global_train_loader, sloss, optimizer)\n",
    "    list_loss.append(loss_avg)\n",
    "\n",
    "    test_model(net_glob, test_loader, sloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model after training on global data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint='./test'\n",
    "torch.save(net_glob.state_dict(), path_checkpoint)\n",
    "#Before fedlearning\n",
    "print('Before Federated Learning')\n",
    "test_model(net_glob, test_loader, sloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Federated Learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Before fedlearning\n",
    "path_checkpoint='./test'\n",
    "\n",
    "net_glob.load_state_dict(torch.load(path_checkpoint))\n",
    "print('Before Federated Learning')\n",
    "#test_model(net_glob, test_loader, sloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_globalnet = copy.deepcopy(net_glob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train local models and update global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensamble at the end!!\n",
    "\n",
    "#Keep track of each sub-model and when validating, \n",
    "#Calculate the loss for each class\n",
    "#save the model with the lowest loss for each class as a separate file\n",
    "\n",
    "#Ensable with Global model and local model\n",
    "\n",
    "\n",
    "#Keep the original, verify with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_users = mnist_iid(global_train_loader.dataset.data, args.num_users)\n",
    "len(dict_users[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[2961 3371 2979 3066 2921 2711 2959 3132 2926 2974]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(global_train_loader.dataset.targets, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)\n",
    "sorted_y = copy.deepcopy(global_train_loader.dataset.targets)\n",
    "sorted_index_y = np.argsort(np.squeeze(sorted_y))\n",
    "\n",
    "class_dist=[]\n",
    "\n",
    "for i in range(args.num_classes):\n",
    "    print(i)\n",
    "    class_dist.append(np.array(sorted_index_y[sum(counts[:i]):sum(counts[:i+1])], dtype=np.int64))\n",
    "    \n",
    "non_iid = np.array(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual = []\n",
    "for j in range(10):\n",
    "    individual.append(np.array_split(class_dist[j], 10))\n",
    "\n",
    "user_dist=[]\n",
    "for i in range(10):\n",
    "    temp=[]\n",
    "    for j in range(10):\n",
    "        temp.append(individual[j][i])\n",
    "        \n",
    "    \n",
    "    user_dist.append((np.concatenate(temp)).astype(np.int64))    \n",
    "    \n",
    "iid=np.array(user_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data loader\n",
    "#local_dataset = CustomDataset(local_x, local_y, transform=transform)\n",
    "#ldr_train = DataLoader(DatasetSplit(local_dataset, class_dist[1]), batch_size=args.local_bs, shuffle=True)\n",
    "#for x,y in ldr_train:\n",
    "    #x=np.squeeze(np.transpose(x, (0,2,3,1)))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_glob = net_glob.state_dict()\n",
    "w_locals = [w_glob for i in range(args.num_users)]\n",
    "#local_dataset = CustomDataset(local_x, local_y, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_train_local(q_l, q_w, idx, loss, model):\n",
    "    \n",
    "    #print(\"asdf11\")\n",
    "    \n",
    "    local = LocalUpdate(args=args, user_num=idx, loss_func=loss, dataset=local_train_loader.dataset, idxs=non_iid[idx])\n",
    "    \n",
    "    #print(\"asdf22\")\n",
    "    print(\"asdf33\")\n",
    "    w, loss = local.train(net=model.to(args.device))\n",
    "    \n",
    "    #print(\"asdf44\")\n",
    "    \n",
    "    #lock.acquire()\n",
    "    q_l.put(loss)\n",
    "    q_w.put(w)\n",
    "    #lock.release()\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    \n",
    "    #return w, loss\n",
    "    #w_locals.append(copy.deepcopy(w))\n",
    "    #loss_locals.append(copy.deepcopy(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_train_local22(q, idx):\n",
    "    print(\"user: \"+str(idx))\n",
    "    q.put(\"asdfasdfasdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.multiprocessing import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.multiprocessing as mp\n",
    "#from torch.multiprocessing import Pool, Process, set_start_method, Queue\n",
    "from multiprocessing import Pool, Process, set_start_method, Queue\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from utils.multiprocessing import work\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "    #torch.set_num_threads(1)\n",
    "    \n",
    "    set_start_method('spawn', force=True)\n",
    "    #ctx = mp.get_context('spawn')\n",
    "\n",
    "    loss_locals=[]\n",
    "    q_l = Queue()\n",
    "\n",
    "    procs=[]\n",
    "\n",
    "    p = Process(target=work, args=(0, q_l))\n",
    "    p.start()\n",
    "    p.join()\n",
    "        \n",
    "        \n",
    "        #w_locals.append(q_w.get())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdfasdfasdf\n"
     ]
    }
   ],
   "source": [
    "print(q_l.get(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(non_iid):\n",
    "    print(non_iid[0])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2961,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(non_iid[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7370, 26615, 26614,  ..., 23367, 15371,  4610])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_iid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.multiprocessing import work, multi_train_local_dif\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf11\n",
      "asdf11\n",
      "ye: 0\n",
      "ye: 1\n",
      "ye22: [ 7370 26615 26614 ... 23367 15371  4610]\n",
      "asdf11\n",
      "ye22: [21808  4826 15174 ...  9455 11518  3440]\n",
      "ye: 2\n",
      "asdf22\n",
      "asdf11\n",
      "ye22: [11011 12102  7682 ... 20523  5095  5716]\n",
      "asdf22\n",
      "ye: 3\n",
      "11\n",
      "11\n",
      "asdf22\n",
      "asdf11\n",
      "ye22: [12493 12382 27569 ...   657  4583  6277]\n",
      "ye: 4\n",
      "asdf22\n",
      "11\n",
      "ye22: [ 1038 15847 28823 ... 23042  9676 29527]\n",
      "11\n",
      "asdf22\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-82:\n",
      "Process Process-81:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process Process-83:\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/utils/multiprocessing.py\", line 25, in multi_train_local_dif\n",
      "    w, loss = local.train(net=model.to(arguemnts.device))\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/models/Update.py\", line 65, in train\n",
      "    loss.backward()\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/utils/multiprocessing.py\", line 25, in multi_train_local_dif\n",
      "    w, loss = local.train(net=model.to(arguemnts.device))\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/models/Update.py\", line 65, in train\n",
      "    loss.backward()\n",
      "Process Process-84:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 98, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 98, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "RuntimeError: Unable to handle autograd's threading in combination with fork-based multiprocessing. See https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork\n",
      "RuntimeError: Unable to handle autograd's threading in combination with fork-based multiprocessing. See https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/utils/multiprocessing.py\", line 25, in multi_train_local_dif\n",
      "    w, loss = local.train(net=model.to(arguemnts.device))\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-85:\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/utils/multiprocessing.py\", line 25, in multi_train_local_dif\n",
      "    w, loss = local.train(net=model.to(arguemnts.device))\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/models/Update.py\", line 65, in train\n",
      "    loss.backward()\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/models/Update.py\", line 65, in train\n",
      "    loss.backward()\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 98, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 98, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "RuntimeError: Unable to handle autograd's threading in combination with fork-based multiprocessing. See https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork\n",
      "RuntimeError: Unable to handle autograd's threading in combination with fork-based multiprocessing. See https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/utils/multiprocessing.py\", line 25, in multi_train_local_dif\n",
      "    w, loss = local.train(net=model.to(arguemnts.device))\n",
      "  File \"/home/imtl/!  personal projects/federated learning/federated-learning/models/Update.py\", line 65, in train\n",
      "    loss.backward()\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 98, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "RuntimeError: Unable to handle autograd's threading in combination with fork-based multiprocessing. See https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # training\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    sloss2 = F.cross_entropy\n",
    "    #sloss2 = SmoothCrossEntropyLoss(smoothing=0.3)\n",
    "\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    #set_start_method('spawn', force=True)\n",
    "    #ctx = mp.get_context('spawn')\n",
    "\n",
    "    loss_locals=[]\n",
    "    w_locals=[]\n",
    "    procs=[]\n",
    "    \n",
    "    \n",
    "    net_glob.share_memory()\n",
    "    \n",
    "    q_l = mp.Queue()\n",
    "    q_w = mp.Queue()\n",
    "    #l = Lock()\n",
    "    \n",
    "    procs=[]\n",
    "    \n",
    "    for i in range(5):\n",
    "\n",
    "        p = mp.Process(target=multi_train_local_dif, args=(q_l, q_w, args, i, sloss2, local_train_loader, non_iid, net_glob))\n",
    "        procs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    \n",
    "    for p in procs:\n",
    "        p.join()\n",
    "        #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.multiprocessing import work, multi_train_local_dif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, Process, set_start_method, Queue\n",
    "import time\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # training\n",
    "    #optimizer2 = optim.SGD(net_glob.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    #optimizer2.zero_grad()\n",
    "    sloss2 = F.cross_entropy\n",
    "    #sloss2 = SmoothCrossEntropyLoss(smoothing=0.3)\n",
    "\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    set_start_method('spawn', force=True)\n",
    "    #ctx = mp.get_context('spawn')\n",
    "\n",
    "    loss_locals=[]\n",
    "    w_locals=[]\n",
    "    procs=[]\n",
    "    \n",
    "    \n",
    "    net_glob.share_memory()\n",
    "    \n",
    "    q_l = Queue()\n",
    "    q_w = Queue()\n",
    "    #l = Lock()\n",
    "    \n",
    "    procs=[]\n",
    "    \n",
    "    for i in range(2):\n",
    "\n",
    "        checkpoint_globalnet11 = copy.deepcopy(net_glob)\n",
    "        checkpoint_globalnet11.share_memory()        \n",
    "        p = Process(target=multi_train_local_dif, args=(q_l, q_w, args, i, sloss2, local_train_loader, non_iid, checkpoint_globalnet11))\n",
    "        procs.append(p)\n",
    "        p.start()\n",
    "    \n",
    "        \n",
    "    #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not q_l.empty():\n",
    "    loss_locals.append(q_l.get())\n",
    "    \n",
    "while not q_w.empty():    \n",
    "    w_locals.append(q_w.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for proc in procs:\n",
    "    proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.012599757862104094, 0.01322289594283547]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_locals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Local user 0 -- Update Epoch: 0 [0/2961 (0%)]\tLoss: 1.855603\n",
      "Local user 0 -- Update Epoch: 0 [100/2961 (3%)]\tLoss: 1.603838\n",
      "Local user 0 -- Update Epoch: 0 [200/2961 (7%)]\tLoss: 1.384777\n",
      "Local user 0 -- Update Epoch: 0 [300/2961 (10%)]\tLoss: 1.463539\n",
      "Local user 0 -- Update Epoch: 0 [400/2961 (13%)]\tLoss: 1.503450\n",
      "Local user 0 -- Update Epoch: 0 [500/2961 (17%)]\tLoss: 1.387912\n",
      "Local user 0 -- Update Epoch: 0 [600/2961 (20%)]\tLoss: 1.429173\n",
      "Local user 0 -- Update Epoch: 0 [700/2961 (24%)]\tLoss: 1.405012\n",
      "Local user 0 -- Update Epoch: 0 [800/2961 (27%)]\tLoss: 1.364068\n",
      "Local user 0 -- Update Epoch: 0 [900/2961 (30%)]\tLoss: 1.419612\n",
      "Local user 0 -- Update Epoch: 0 [1000/2961 (34%)]\tLoss: 1.369678\n",
      "Local user 0 -- Update Epoch: 0 [1100/2961 (37%)]\tLoss: 1.424651\n",
      "Local user 0 -- Update Epoch: 0 [1200/2961 (40%)]\tLoss: 1.397029\n",
      "Local user 0 -- Update Epoch: 0 [1300/2961 (44%)]\tLoss: 1.404231\n",
      "Local user 0 -- Update Epoch: 0 [1400/2961 (47%)]\tLoss: 1.361458\n",
      "Local user 0 -- Update Epoch: 0 [1500/2961 (51%)]\tLoss: 1.423403\n",
      "Local user 0 -- Update Epoch: 0 [1600/2961 (54%)]\tLoss: 1.382034\n",
      "Local user 0 -- Update Epoch: 0 [1700/2961 (57%)]\tLoss: 1.366475\n",
      "Local user 0 -- Update Epoch: 0 [1800/2961 (61%)]\tLoss: 1.377216\n",
      "Local user 0 -- Update Epoch: 0 [1900/2961 (64%)]\tLoss: 1.342664\n",
      "Local user 0 -- Update Epoch: 0 [2000/2961 (67%)]\tLoss: 1.330659\n",
      "Local user 0 -- Update Epoch: 0 [2100/2961 (71%)]\tLoss: 1.320536\n",
      "Local user 0 -- Update Epoch: 0 [2200/2961 (74%)]\tLoss: 1.331217\n",
      "Local user 0 -- Update Epoch: 0 [2300/2961 (77%)]\tLoss: 1.323138\n",
      "Local user 0 -- Update Epoch: 0 [2400/2961 (81%)]\tLoss: 1.324404\n",
      "Local user 0 -- Update Epoch: 0 [2500/2961 (84%)]\tLoss: 1.341994\n",
      "Local user 0 -- Update Epoch: 0 [2600/2961 (88%)]\tLoss: 1.349145\n",
      "Local user 0 -- Update Epoch: 0 [2700/2961 (91%)]\tLoss: 1.340045\n",
      "Local user 0 -- Update Epoch: 0 [2800/2961 (94%)]\tLoss: 1.335896\n",
      "Local user 0 -- Update Epoch: 0 [2900/2961 (98%)]\tLoss: 1.361423\n",
      "11\n",
      "Local user 1 -- Update Epoch: 0 [0/3371 (0%)]\tLoss: 1.897630\n",
      "Local user 1 -- Update Epoch: 0 [100/3371 (3%)]\tLoss: 1.528684\n",
      "Local user 1 -- Update Epoch: 0 [200/3371 (6%)]\tLoss: 1.360813\n",
      "Local user 1 -- Update Epoch: 0 [300/3371 (9%)]\tLoss: 1.454417\n",
      "Local user 1 -- Update Epoch: 0 [400/3371 (12%)]\tLoss: 1.420106\n",
      "Local user 1 -- Update Epoch: 0 [500/3371 (15%)]\tLoss: 1.451827\n",
      "Local user 1 -- Update Epoch: 0 [600/3371 (18%)]\tLoss: 1.395170\n",
      "Local user 1 -- Update Epoch: 0 [700/3371 (21%)]\tLoss: 1.490448\n",
      "Local user 1 -- Update Epoch: 0 [800/3371 (24%)]\tLoss: 1.371752\n",
      "Local user 1 -- Update Epoch: 0 [900/3371 (27%)]\tLoss: 1.374112\n",
      "Local user 1 -- Update Epoch: 0 [1000/3371 (30%)]\tLoss: 1.412713\n",
      "Local user 1 -- Update Epoch: 0 [1100/3371 (33%)]\tLoss: 1.368963\n",
      "Local user 1 -- Update Epoch: 0 [1200/3371 (36%)]\tLoss: 1.386536\n",
      "Local user 1 -- Update Epoch: 0 [1300/3371 (38%)]\tLoss: 1.340908\n",
      "Local user 1 -- Update Epoch: 0 [1400/3371 (41%)]\tLoss: 1.351090\n",
      "Local user 1 -- Update Epoch: 0 [1500/3371 (44%)]\tLoss: 1.399417\n",
      "Local user 1 -- Update Epoch: 0 [1600/3371 (47%)]\tLoss: 1.352814\n",
      "Local user 1 -- Update Epoch: 0 [1700/3371 (50%)]\tLoss: 1.385276\n",
      "Local user 1 -- Update Epoch: 0 [1800/3371 (53%)]\tLoss: 1.312501\n",
      "Local user 1 -- Update Epoch: 0 [1900/3371 (56%)]\tLoss: 1.372662\n",
      "Local user 1 -- Update Epoch: 0 [2000/3371 (59%)]\tLoss: 1.379508\n",
      "Local user 1 -- Update Epoch: 0 [2100/3371 (62%)]\tLoss: 1.378549\n",
      "Local user 1 -- Update Epoch: 0 [2200/3371 (65%)]\tLoss: 1.342836\n",
      "Local user 1 -- Update Epoch: 0 [2300/3371 (68%)]\tLoss: 1.359849\n",
      "Local user 1 -- Update Epoch: 0 [2400/3371 (71%)]\tLoss: 1.375661\n",
      "Local user 1 -- Update Epoch: 0 [2500/3371 (74%)]\tLoss: 1.335628\n",
      "Local user 1 -- Update Epoch: 0 [2600/3371 (77%)]\tLoss: 1.327062\n",
      "Local user 1 -- Update Epoch: 0 [2700/3371 (80%)]\tLoss: 1.337957\n",
      "Local user 1 -- Update Epoch: 0 [2800/3371 (83%)]\tLoss: 1.313376\n",
      "Local user 1 -- Update Epoch: 0 [2900/3371 (86%)]\tLoss: 1.367253\n",
      "Local user 1 -- Update Epoch: 0 [3000/3371 (89%)]\tLoss: 1.364170\n",
      "Local user 1 -- Update Epoch: 0 [3100/3371 (92%)]\tLoss: 1.324678\n",
      "Local user 1 -- Update Epoch: 0 [3200/3371 (95%)]\tLoss: 1.370186\n",
      "Local user 1 -- Update Epoch: 0 [3300/3371 (98%)]\tLoss: 1.319634\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to builtin_function_or_method.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4969319b565d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcheckpoint_globalnet22\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_glob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mloss_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_locals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_locals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Round {:3d}, Average loss {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_globalnet22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to builtin_function_or_method.__format__"
     ]
    }
   ],
   "source": [
    "checkpoint_globalnet22 = copy.deepcopy(net_glob)\n",
    "\n",
    "checkpoint_globalnet22.train()\n",
    "loss_locals = []\n",
    "w_locals = []\n",
    "for idx in range(args.num_users):\n",
    "\n",
    "\n",
    "    local = LocalUpdate(args=args, user_num=idx, loss_func=sloss, dataset=local_train_loader.dataset, idxs=non_iid[idx])\n",
    "    w, loss = local.train(net=copy.deepcopy(checkpoint_globalnet22).to(args.device))\n",
    "    w_locals.append(copy.deepcopy(w))\n",
    "    loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "w_glob = FedAvg(w_locals)\n",
    "checkpoint_globalnet22.load_state_dict(w_glob)\n",
    "loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "\n",
    "test_model(checkpoint_globalnet22, test_loader, sloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.00230 \n",
      "Accuracy: 1365/10000 (13.65%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(checkpoint_globalnet22, test_loader, sloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Local user 0 -- Update Epoch: 0 [0/2961 (0%)]\tLoss: 2.062182\n",
      "Local user 0 -- Update Epoch: 0 [100/2961 (3%)]\tLoss: 0.210519\n",
      "Local user 0 -- Update Epoch: 0 [200/2961 (7%)]\tLoss: 0.002557\n",
      "Local user 0 -- Update Epoch: 0 [300/2961 (10%)]\tLoss: 0.024340\n",
      "Local user 0 -- Update Epoch: 0 [400/2961 (13%)]\tLoss: 0.089091\n",
      "Local user 0 -- Update Epoch: 0 [500/2961 (17%)]\tLoss: 0.000027\n",
      "Local user 0 -- Update Epoch: 0 [600/2961 (20%)]\tLoss: 0.044405\n",
      "Local user 0 -- Update Epoch: 0 [700/2961 (24%)]\tLoss: 0.002056\n",
      "Local user 0 -- Update Epoch: 0 [800/2961 (27%)]\tLoss: 0.001303\n",
      "Local user 0 -- Update Epoch: 0 [900/2961 (30%)]\tLoss: 0.000543\n",
      "Local user 0 -- Update Epoch: 0 [1000/2961 (34%)]\tLoss: 0.002419\n",
      "Local user 0 -- Update Epoch: 0 [1100/2961 (37%)]\tLoss: 0.000297\n",
      "Local user 0 -- Update Epoch: 0 [1200/2961 (40%)]\tLoss: 0.000001\n",
      "Local user 0 -- Update Epoch: 0 [1300/2961 (44%)]\tLoss: 0.001437\n",
      "Local user 0 -- Update Epoch: 0 [1400/2961 (47%)]\tLoss: 0.000051\n",
      "Local user 0 -- Update Epoch: 0 [1500/2961 (51%)]\tLoss: 0.004101\n",
      "Local user 0 -- Update Epoch: 0 [1600/2961 (54%)]\tLoss: 0.000051\n",
      "Local user 0 -- Update Epoch: 0 [1700/2961 (57%)]\tLoss: 0.000000\n",
      "Local user 0 -- Update Epoch: 0 [1800/2961 (61%)]\tLoss: 0.000035\n",
      "Local user 0 -- Update Epoch: 0 [1900/2961 (64%)]\tLoss: 0.000004\n",
      "Local user 0 -- Update Epoch: 0 [2000/2961 (67%)]\tLoss: 0.001116\n",
      "Local user 0 -- Update Epoch: 0 [2100/2961 (71%)]\tLoss: 0.000015\n",
      "Local user 0 -- Update Epoch: 0 [2200/2961 (74%)]\tLoss: 0.000008\n",
      "Local user 0 -- Update Epoch: 0 [2300/2961 (77%)]\tLoss: 0.000001\n",
      "Local user 0 -- Update Epoch: 0 [2400/2961 (81%)]\tLoss: 0.000001\n",
      "Local user 0 -- Update Epoch: 0 [2500/2961 (84%)]\tLoss: 0.000000\n",
      "Local user 0 -- Update Epoch: 0 [2600/2961 (88%)]\tLoss: 0.000149\n",
      "Local user 0 -- Update Epoch: 0 [2700/2961 (91%)]\tLoss: 0.000003\n",
      "Local user 0 -- Update Epoch: 0 [2800/2961 (94%)]\tLoss: 0.000061\n",
      "Local user 0 -- Update Epoch: 0 [2900/2961 (98%)]\tLoss: 0.000000\n",
      "11\n",
      "Local user 1 -- Update Epoch: 0 [0/3371 (0%)]\tLoss: 1.783881\n",
      "Local user 1 -- Update Epoch: 0 [100/3371 (3%)]\tLoss: 0.124908\n",
      "Local user 1 -- Update Epoch: 0 [200/3371 (6%)]\tLoss: 0.041978\n",
      "Local user 1 -- Update Epoch: 0 [300/3371 (9%)]\tLoss: 0.029348\n",
      "Local user 1 -- Update Epoch: 0 [400/3371 (12%)]\tLoss: 0.000734\n",
      "Local user 1 -- Update Epoch: 0 [500/3371 (15%)]\tLoss: 0.006661\n",
      "Local user 1 -- Update Epoch: 0 [600/3371 (18%)]\tLoss: 0.021146\n",
      "Local user 1 -- Update Epoch: 0 [700/3371 (21%)]\tLoss: 0.012909\n",
      "Local user 1 -- Update Epoch: 0 [800/3371 (24%)]\tLoss: 0.000219\n",
      "Local user 1 -- Update Epoch: 0 [900/3371 (27%)]\tLoss: 0.000388\n",
      "Local user 1 -- Update Epoch: 0 [1000/3371 (30%)]\tLoss: 0.001531\n",
      "Local user 1 -- Update Epoch: 0 [1100/3371 (33%)]\tLoss: 0.005434\n",
      "Local user 1 -- Update Epoch: 0 [1200/3371 (36%)]\tLoss: 0.000110\n",
      "Local user 1 -- Update Epoch: 0 [1300/3371 (38%)]\tLoss: 0.000709\n",
      "Local user 1 -- Update Epoch: 0 [1400/3371 (41%)]\tLoss: 0.000646\n",
      "Local user 1 -- Update Epoch: 0 [1500/3371 (44%)]\tLoss: 0.002482\n",
      "Local user 1 -- Update Epoch: 0 [1600/3371 (47%)]\tLoss: 0.000001\n",
      "Local user 1 -- Update Epoch: 0 [1700/3371 (50%)]\tLoss: 0.000106\n",
      "Local user 1 -- Update Epoch: 0 [1800/3371 (53%)]\tLoss: 0.000107\n",
      "Local user 1 -- Update Epoch: 0 [1900/3371 (56%)]\tLoss: 0.001865\n",
      "Local user 1 -- Update Epoch: 0 [2000/3371 (59%)]\tLoss: 0.000011\n",
      "Local user 1 -- Update Epoch: 0 [2100/3371 (62%)]\tLoss: 0.000025\n",
      "Local user 1 -- Update Epoch: 0 [2200/3371 (65%)]\tLoss: 0.000010\n",
      "Local user 1 -- Update Epoch: 0 [2300/3371 (68%)]\tLoss: 0.000046\n",
      "Local user 1 -- Update Epoch: 0 [2400/3371 (71%)]\tLoss: 0.000120\n",
      "Local user 1 -- Update Epoch: 0 [2500/3371 (74%)]\tLoss: 0.000093\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6f9dbaae721b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_train_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_iid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_glob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mw_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/!  personal projects/federated learning/federated-learning/models/Update.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, net)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;31m#print(\"77\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/imtl/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/imtl/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(args.epochs):\n",
    "    net_glob.train()\n",
    "    loss_locals = []\n",
    "    w_locals = []\n",
    "    #m = max(int(args.frac * args.num_users), 1)\n",
    "    #idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    \n",
    "    for idx in range(args.num_users):\n",
    "\n",
    "        \n",
    "        local = LocalUpdate(args=args, user_num=idx, loss_func=sloss, dataset=local_train_loader.dataset, idxs=non_iid[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    \n",
    "    w_glob = FedAvg(w_locals)\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "    \n",
    "    test_model(net_glob, test_loader, sloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model on local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Federated Learning -- checkpoint\n",
      "\n",
      "Test set: Average loss: 0.01016 \n",
      "Accuracy: 9647/10000 (96.47%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#After fedlearning\n",
    "print('After Federated Learning -- checkpoint')\n",
    "test_model(checkpoint_globalnet, test_loader, sloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Federated Learning\n",
      "\n",
      "Test set: Average loss: 0.02773 \n",
      "Accuracy: 9574/10000 (95.74%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#After fedlearning\n",
    "print('After Federated Learning')\n",
    "test_model(net_glob, test_loader, sloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot acc curve\n",
    "f, axarr = plt.subplots(1,2, figsize=(16,5))\n",
    "axarr[0].plot(range(len(total_acc))[:args.epochs], total_acc[:args.epochs])\n",
    "axarr[0].set_title('Test accuracy on global data')\n",
    "#axarr[0].set_xlim([80, 98])\n",
    "axarr[0].set_ylim([80, 98])\n",
    "axarr[1].plot(range(len(total_acc))[:args.epochs], total_acc[args.epochs:])\n",
    "axarr[1].set_title('Test accuracy after training on Local Data')\n",
    "#axarr[1].set_xlim([80, 98])\n",
    "axarr[1].set_ylim([80, 98])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
